import configparser
import boto3
import os
import time  # ëŒ€ê¸° ì‹œê°„ì„ ìœ„í•´ ì¶”ê°€
import sys

def load_config(config_path='pipeline.conf'):
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    parser = configparser.ConfigParser()
    
    if not os.path.exists(config_path):
        print(f"Error: ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ -> {config_path}")
        sys.exit(1) # íŒŒì¼ ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ

    try:
        parser.read(config_path)
        config = parser["redshift_copy"] # ì„¹ì…˜ ì „ì²´ë¥¼ ê°€ì ¸ì˜´
        
        # í•„ìš”í•œ í‚¤ê°’ë“¤ì´ ë‹¤ ìˆëŠ”ì§€ í™•ì¸
        required_keys = ["region_name", "workgroup_name", "database_name", "db_user", 
                         "b_account_iam_role_arn", "s3_bucket_name", "s3_file_path", "target_table"]
        
        # ì„¤ì •ê°’ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        conf_data = {key: config.get(key) for key in required_keys}
        
        # AWS ìê²©ì¦ëª…ë„ í•¨ê»˜ ë¡œë“œ
        creds = parser["B_aws_credentials"]
        conf_data['aws_access_key'] = creds.get('access_key')
        conf_data['aws_secret_key'] = creds.get('secret_key')
        
        return conf_data
    
    except Exception as e:
        print(f"ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        sys.exit(1)

def check_query_status(client, query_id):
    """ì¿¼ë¦¬ê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘... (ID: {query_id})")
    
    while True:
        response = client.describe_statement(Id=query_id)
        status = response['Status']
        
        if status == 'FINISHED':
            print("âœ… ì„±ê³µ! (ë°ì´í„° ì ì¬ ì™„ë£Œ)")
            return True
        elif status == 'FAILED':
            print(f"âŒ ì‹¤íŒ¨! ì—ëŸ¬ ë©”ì‹œì§€: {response['Error']}")
            return False
        elif status == 'ABORTED':
            print("ğŸš« ì·¨ì†Œë¨.")
            return False
        
        # ì•„ì§ ì‹¤í–‰ ì¤‘ì´ë©´ 2ì´ˆ ì‰¬ê³  ë‹¤ì‹œ í™•ì¸
        time.sleep(2)

def copy_s3_to_redshift():
    # 1. ì„¤ì • ë¡œë“œ
    conf = load_config()

    # 2. Redshift Data API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = boto3.client('redshift-data', 
                          region_name=conf['region_name'],
                          aws_access_key_id=conf['aws_access_key'],      # ì—¬ê¸°!
                          aws_secret_access_key=conf['aws_secret_key'])

    # 3. COPY ëª…ë ¹ì–´ ìƒì„± (ìˆ˜ì •ë¨)
    # - IAM_ROLE: Bê³„ì • ì—­í•  í•˜ë‚˜ë§Œ ì‚¬ìš©
    # - REGION: S3 ë¦¬ì „ì„ ëª…ì‹œ (ì—¬ê¸°ì„œëŠ” Redshiftì™€ ê°™ë‹¤ê³  ê°€ì •í•˜ê³  ì„¤ì •ê°’ ì‚¬ìš©)
    copy_command = f"""
        COPY {conf['target_table']}
        FROM 's3://{conf['s3_bucket_name']}/{conf['s3_file_path']}'
        IAM_ROLE '{conf['b_account_iam_role_arn']}'
        REGION '{conf['region_name']}'
        CSV
        IGNOREHEADER 1;
    """

    try:
        print("ğŸš€ COPY ëª…ë ¹ ì „ì†¡ ì‹œì‘...")
        
        # 4. ì‹¤í–‰ (ë¹„ë™ê¸°)
        response = client.execute_statement(
            WorkgroupName=conf['workgroup_name'],
            Database=conf['database_name'],
            DbUser=conf['db_user'], # Secrets Managerë¥¼ ì“´ë‹¤ë©´ SecretArnìœ¼ë¡œ êµì²´ ê¶Œì¥
            Sql=copy_command
        )
        
        query_id = response['Id']
        
        # 5. ê²°ê³¼ ëŒ€ê¸° ë° í™•ì¸ (ì¤‘ìš”!)
        check_query_status(client, query_id)

    except Exception as e:
        print(f"ğŸ”¥ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    copy_s3_to_redshift()